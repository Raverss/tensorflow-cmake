{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from user_ops import ft_inverse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', '..', 'keras_frac'))\n",
    "from fractional_maxpooling import FractionalPooling2D\n",
    "import resnet\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "init = \"he_uniform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_net:\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='r2c2elu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def train(self, batch_size, epochs, datagen=None, train_data=None, callbacks=None):\n",
    "        if datagen is None and train_data is None:\n",
    "            print('neither data or generator was passed')\n",
    "        elif datagen is None and train_data is not None:\n",
    "            print('training on array data, network type:', type(self).__name__)\n",
    "            self.history = self.model.fit(x = train_data[0], y = train_data[1], validation_data=train_data[2:], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        else:\n",
    "            print('training on datagen data, network type:', type(self).__name__)\n",
    "            self.history = self.model.fit_generator(datagen, validation_data=train_data[2:], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        self.weights = self.model.get_weights()\n",
    "    \n",
    "    def restart_session(self):\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.weights)\n",
    "\n",
    "        \n",
    "class frac_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        ratio = 1.35 # 1.33 blocks=3\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "class ft_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        strides=(math.sqrt(2), math.sqrt(2))\n",
    "        pool_size=(math.sqrt(2)*2, math.sqrt(2)*2)\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        \n",
    "class avg_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        pool_size=2\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "        self.model.add(keras.layers.AveragePooling2D(pool_size=(pool_size, pool_size), strides=(2,2), padding='same'))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same', kernel_initializer=init))\n",
    "            self.model.add(keras.layers.AveragePooling2D(pool_size=(pool_size, pool_size), strides=(2,2), padding='same'))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 100\n",
    "iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (100, 32, 32, 2)          56        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (100, 23, 23, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (100, 23, 23, 2)          38        \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (100, 16, 16, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (100, 16, 16, 4)          76        \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (100, 11, 11, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (100, 11, 11, 4)          148       \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (100, 8, 8, 4)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 10)                 2570      \n",
      "=================================================================\n",
      "Total params: 2,888\n",
      "Trainable params: 2,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "training on array data, network type: ft_pool_net\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 6656400 values, but the requested shape requires a multiple of 256\n\t [[{{node flatten_1/Reshape}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-280e4704b11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mft_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_pool_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mft_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mft_net_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mft_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mft_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8beff53f6466>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, datagen, train_data, callbacks)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdatagen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training on array data, network type:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training on datagen data, network type:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 6656400 values, but the requested shape requires a multiple of 256\n\t [[{{node flatten_1/Reshape}}]]"
     ]
    }
   ],
   "source": [
    "ft_net_h = []\n",
    "path = os.path.join('results', 'mine_with_div')\n",
    "for _ in range(iter):\n",
    "    ft_net = ft_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "    ft_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "    ft_net_h.append([ft_net.get_history().history['accuracy'], ft_net.get_history().history['val_accuracy']])\n",
    "    ft_net.restart_session()\n",
    "with open(os.path.join(path,'ft_net_h.pkl'), 'wb') as f:\n",
    "    pickle.dump(ft_net_h, f)\n",
    "'''\n",
    "avg_net_h = []\n",
    "for _ in range(iter):\n",
    "    avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "    avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "    avg_net_h.append([avg_net.get_history().history['accuracy'], avg_net.get_history().history['val_accuracy']])\n",
    "    avg_net.restart_session()\n",
    "with open(os.path.join(path,'avg_net_h.pkl'), 'wb') as f:\n",
    "    pickle.dump(avg_net_h, f)\n",
    "\n",
    "max_net_h = []\n",
    "for _ in range(iter):\n",
    "    max_net = max_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "    max_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "    max_net_h.append([max_net.get_history().history['accuracy'], max_net.get_history().history['val_accuracy']])\n",
    "    max_net.restart_session()\n",
    "with open(os.path.join(path,'max_net_h.pkl')) as f:\n",
    "    pickle.dump(max_net_h, f)\n",
    "\n",
    "frac_net_h = []\n",
    "for _ in range(iter):\n",
    "    frac_net = frac_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "    frac_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "    frac_net_h.append([frac_net.get_history().history['accuracy'], frac_net.get_history().history['val_accuracy']])\n",
    "    frac_net.restart_session()\n",
    "with open(os.path.join(path,'frac_net_h.pkl'), 'wb') as f:\n",
    "    pickle.dump(frac_net_h, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline best 2x max pool = 53.19%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','max_net_h.pkl'), 'rb') as f:\n",
    "    max_net_h = pickle.load(f)\n",
    "plt.figure(figsize=(15,7))\n",
    "arr = np.array(max_net_h)\n",
    "plt.plot(np.mean(arr[:,0], axis=0), 'r')\n",
    "plt.plot(np.max(arr[:,0], axis=0), 'r--')\n",
    "\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','frac_net_h.pkl'), 'rb') as f:\n",
    "    frac_net_h = pickle.load(f)\n",
    "plt.figure(figsize=(15,7))\n",
    "arr = np.array(frac_net_h)\n",
    "plt.plot(np.mean(arr[:,0], axis=0), 'g')\n",
    "plt.plot(np.max(arr[:,0], axis=0), 'g--')\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','ft_net_h.pkl'), 'rb') as f:\n",
    "    ft_net_h = pickle.load(f)\n",
    "plt.figure(figsize=(15,7))\n",
    "arr = np.array(ft_net_h)\n",
    "plt.plot(np.mean(arr[:,0], axis=0), 'b')\n",
    "plt.plot(np.max(arr[:,0], axis=0), 'b--')\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','avg_net_h.pkl'), 'rb') as f:\n",
    "    avg_net_h = pickle.load(f)\n",
    "plt.figure(figsize=(15,7))\n",
    "arr = np.array(avg_net_h)\n",
    "plt.plot(np.mean(arr[:,0], axis=0), 'c')\n",
    "plt.plot(np.max(arr[:,0], axis=0), 'c--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','max_net_h.pkl'), 'rb') as f:\n",
    "    max_net_h = pickle.load(f)\n",
    "plt.figure(figsize=(15,7))\n",
    "arr = np.array(max_net_h)\n",
    "arr = np.mean(arr[:,1], axis=0)\n",
    "plt.plot(arr, 'r')\n",
    "for v in range(len(arr)):\n",
    "  print(\"({},{:.3f})\".format(v, arr[v]), end=' ')\n",
    "print()\n",
    "\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','frac_net_h.pkl'), 'rb') as f:\n",
    "    frac_net_h = pickle.load(f)\n",
    "arr = np.array(frac_net_h)\n",
    "arr = np.mean(arr[:,1], axis=0)\n",
    "plt.plot(arr, 'g')\n",
    "for v in range(len(arr)):\n",
    "  print(\"({},{:.3f})\".format(v, arr[v]), end=' ')\n",
    "print()\n",
    "\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','ft_net_h.pkl'), 'rb') as f:\n",
    "    ft_net_h = pickle.load(f)\n",
    "arr = np.array(ft_net_h)\n",
    "arr = np.mean(arr[:,1], axis=0)\n",
    "plt.plot(arr, 'b')\n",
    "for v in range(len(arr)):\n",
    "  print(\"({},{:.3f})\".format(v, arr[v]), end=' ')\n",
    "print()\n",
    "\n",
    "with open(os.path.join('/mnt/F0CC2854CC28177E/repos/workIRAFM/tensorflow-cmake/custom_op/results/mine_without_div','avg_net_h.pkl'), 'rb') as f:\n",
    "    avg_net_h = pickle.load(f)\n",
    "arr = np.array(avg_net_h)\n",
    "arr = np.mean(arr[:,1], axis=0)\n",
    "plt.plot(arr, 'c')\n",
    "for v in range(len(arr)):\n",
    "  print(\"({},{:.3f})\".format(v, arr[v]), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ftpoolresnet = resnet.FtPoolingResnet(n=3)\n",
    "ftpoolresnet.build_network()\n",
    "ftpoolresnet.train()\n",
    "ftpoolresnet.save_history('resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(len(ftpoolresnet.history.history['val_accuracy'])):\n",
    "  print(\"({},{:.3f})\".format(v, ftpoolresnet.history.history['val_accuracy'][v]), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/F0CC2854CC28177E/repos/workIRAFM/lenetft/ftnet_with_permutations/history_jsons_8_kernels/resnet20.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sum = np.zeros(200, dtype=np.float32)\n",
    "for s in data['val_accuracy']:\n",
    "    _sum += np.array(s)\n",
    "_sum /= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(len(_sum)):\n",
    "  print(\"({},{:.3f})\".format(v, _sum[v]), end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('tf1_13': conda)",
   "language": "python",
   "name": "python37064bittf113conda3a6e91fe0a544785b67b740c9ef5643c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
